"""
ì›¹ ìŠ¤í¬ë ˆì´í¼ í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ

ì½”ë“œ ë¦¬ë·°ì—ì„œ ë°œê²¬ëœ ì£¼ìš” ì´ìŠˆì— ëŒ€í•œ í…ŒìŠ¤íŠ¸:
1. HTML íŒŒì‹± ì˜¤ë¥˜ (ì˜ëª»ëœ í˜•ì‹ì˜ HTML)
2. URL ê²€ì¦ ëˆ„ë½ (SSRF ì·¨ì•½ì )
3. SSL ê²€ì¦ ëˆ„ë½
"""

import pytest
import requests
import socket
from unittest.mock import Mock, patch, MagicMock
from bs4 import BeautifulSoup

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from examples.web_scraper import (
    WebScraper,
    Article,
    extract_links,
    extract_images
)


class TestHTMLParsingErrors:
    """HTML íŒŒì‹± ì˜¤ë¥˜ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_parse_malformed_html_with_random_text(self):
        """
        í…ŒìŠ¤íŠ¸: ì˜ëª»ëœ í˜•ì‹ì˜ HTML (ì„ì˜ì˜ í…ìŠ¤íŠ¸ í¬í•¨) íŒŒì‹±

        ì‹œë‚˜ë¦¬ì˜¤: ë¼ì¸ 226ì˜ "sdsdsd"ì™€ ê°™ì€ ì˜ëª»ëœ í…ìŠ¤íŠ¸ê°€ HTMLì— í¬í•¨ëœ ê²½ìš°
        ì˜ˆìƒ ê²°ê³¼: íŒŒì„œê°€ ì—ëŸ¬ ì—†ì´ ì²˜ë¦¬í•˜ê³  ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ì¶œ
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        malformed_html = """
        <html>
            <body>
                sdsdsd
                <article class="post">
                    <h2 class="title">Valid Article</h2>
                    <a href="/article/1">Link</a>
                    <p class="summary">Valid summary</p>
                </article>
                random text here
                <article class="post">
                    <h2 class="title">Another Article</h2>
                    random inline text
                    <a href="/article/2">Link 2</a>
                </article>
            </body>
        </html>
        """

        # Act (ì‹¤í–‰)
        articles = scraper.parse_articles(malformed_html)

        # Assert (ê²€ì¦)
        assert len(articles) == 2, "ì˜ëª»ëœ í…ìŠ¤íŠ¸ê°€ ìˆì–´ë„ ìœ íš¨í•œ ê¸°ì‚¬ëŠ” íŒŒì‹±ë˜ì–´ì•¼ í•¨"
        assert articles[0].title == "Valid Article"
        assert articles[1].title == "Another Article"

    def test_parse_completely_broken_html(self):
        """
        í…ŒìŠ¤íŠ¸: ì™„ì „íˆ ê¹¨ì§„ HTML íŒŒì‹±

        ì‹œë‚˜ë¦¬ì˜¤: ë‹«íˆì§€ ì•Šì€ íƒœê·¸, ì˜ëª»ëœ ì¤‘ì²© êµ¬ì¡°
        ì˜ˆìƒ ê²°ê³¼: BeautifulSoupì´ ìµœì„ ì„ ë‹¤í•´ íŒŒì‹±í•˜ê³  ì—ëŸ¬ ì—†ì´ ì²˜ë¦¬
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        broken_html = """
        <html>
            <body>
                <article class="post">
                    <h2 class="title">Unclosed Article
                    <a href="/test">Link
                    <p class="summary">Unclosed paragraph
                <article class="post">
                    </h2><a href="/broken"></a><h2 class="title">Broken Nesting</h2>
                </article>
            </body>
        """

        # Act (ì‹¤í–‰)
        # ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šê³  ì •ìƒì ìœ¼ë¡œ íŒŒì‹±ë˜ì–´ì•¼ í•¨
        articles = scraper.parse_articles(broken_html)

        # Assert (ê²€ì¦)
        assert isinstance(articles, list), "ê¹¨ì§„ HTMLë„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•´ì•¼ í•¨"
        # BeautifulSoupì€ ê¹¨ì§„ HTMLì„ ë³µêµ¬í•˜ë¯€ë¡œ ì¼ë¶€ articleì„ ì°¾ì„ ìˆ˜ ìˆìŒ

    def test_parse_html_with_missing_required_elements(self):
        """
        í…ŒìŠ¤íŠ¸: í•„ìˆ˜ ìš”ì†Œê°€ ì—†ëŠ” HTML íŒŒì‹±

        ì‹œë‚˜ë¦¬ì˜¤: titleì´ë‚˜ linkê°€ ì—†ëŠ” article íƒœê·¸
        ì˜ˆìƒ ê²°ê³¼: í•´ë‹¹ articleì€ ê±´ë„ˆë›°ê³  ìœ íš¨í•œ articleë§Œ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        incomplete_html = """
        <html>
            <body>
                <article class="post">
                    <h2 class="title">Only Title</h2>
                </article>
                <article class="post">
                    <a href="/only-link">Only Link</a>
                </article>
                <article class="post">
                    <h2 class="title">Complete Article</h2>
                    <a href="/complete">Link</a>
                </article>
            </body>
        </html>
        """

        # Act (ì‹¤í–‰)
        articles = scraper.parse_articles(incomplete_html)

        # Assert (ê²€ì¦)
        assert len(articles) == 1, "í•„ìˆ˜ ìš”ì†Œê°€ ëª¨ë‘ ìˆëŠ” articleë§Œ ë°˜í™˜ë˜ì–´ì•¼ í•¨"
        assert articles[0].title == "Complete Article"

    def test_parse_empty_html(self):
        """
        í…ŒìŠ¤íŠ¸: ë¹ˆ HTML íŒŒì‹±

        ì‹œë‚˜ë¦¬ì˜¤: ë¹„ì–´ìˆê±°ë‚˜ articleì´ ì—†ëŠ” HTML
        ì˜ˆìƒ ê²°ê³¼: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        assert scraper.parse_articles("") == []
        assert scraper.parse_articles("<html></html>") == []
        assert scraper.parse_articles("<html><body></body></html>") == []

    def test_parse_html_with_special_characters(self):
        """
        í…ŒìŠ¤íŠ¸: íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ëœ HTML íŒŒì‹±

        ì‹œë‚˜ë¦¬ì˜¤: HTML ì—”í‹°í‹°, ìœ ë‹ˆì½”ë“œ, íŠ¹ìˆ˜ ê¸°í˜¸
        ì˜ˆìƒ ê²°ê³¼: íŠ¹ìˆ˜ ë¬¸ìê°€ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±ë¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        special_html = """
        <html>
            <body>
                <article class="post">
                    <h2 class="title">&lt;Script&gt; &amp; "Quotes" &#39;Apostrophe&#39;</h2>
                    <a href="/article/1">Link</a>
                    <p class="summary">í•œê¸€ ãƒ†ã‚¹ãƒˆ ä¸­æ–‡ ğŸš€ Emoji</p>
                </article>
            </body>
        </html>
        """

        # Act (ì‹¤í–‰)
        articles = scraper.parse_articles(special_html)

        # Assert (ê²€ì¦)
        assert len(articles) == 1
        assert "&lt;" in articles[0].title or "<" in articles[0].title
        assert "í•œê¸€" in articles[0].summary


class TestURLValidationSSRF:
    """URL ê²€ì¦ ë° SSRF ì·¨ì•½ì  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_reject_localhost_url(self):
        """
        í…ŒìŠ¤íŠ¸: localhost URL ê±°ë¶€

        ì‹œë‚˜ë¦¬ì˜¤: SSRF ê³µê²©ì„ í†µí•´ localhostì— ì ‘ê·¼ ì‹œë„
        ì˜ˆìƒ ê²°ê³¼: ValueError ë°œìƒí•˜ì—¬ localhost ì ‘ê·¼ ì°¨ë‹¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        # URL ê²€ì¦ì´ êµ¬í˜„ë˜ì–´ localhost ì ‘ê·¼ì´ ì°¨ë‹¨ë¨
        with pytest.raises(ValueError, match="ë‚´ë¶€ í˜¸ìŠ¤íŠ¸ ì ‘ê·¼ ê¸ˆì§€"):
            scraper.fetch_page("http://localhost/admin")

    def test_reject_internal_ip_addresses(self):
        """
        í…ŒìŠ¤íŠ¸: ë‚´ë¶€ IP ì£¼ì†Œ ê±°ë¶€

        ì‹œë‚˜ë¦¬ì˜¤: SSRF ê³µê²©ì„ í†µí•´ ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ IPì— ì ‘ê·¼ ì‹œë„
        ì˜ˆìƒ ê²°ê³¼: 192.168.x.x, 10.x.x.x, 127.x.x.x ë“± ë‚´ë¶€ IP ê±°ë¶€
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        dangerous_urls = [
            ("http://127.0.0.1/admin", "ë£¨í”„ë°± IP ì ‘ê·¼ ê¸ˆì§€"),
            ("http://192.168.1.1/config", "ë‚´ë¶€ IP ì ‘ê·¼ ê¸ˆì§€"),
            ("http://10.0.0.1/internal", "ë‚´ë¶€ IP ì ‘ê·¼ ê¸ˆì§€"),
            ("http://172.16.0.1/secret", "ë‚´ë¶€ IP ì ‘ê·¼ ê¸ˆì§€"),
            # 169.254.169.254ëŠ” is_privateë¡œ ë¨¼ì € ê±¸ë¦¼ (link-localì€ privateì˜ ì¼ì¢…)
            ("http://169.254.169.254/metadata", "ë‚´ë¶€ IP ì ‘ê·¼ ê¸ˆì§€"),
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        for url, expected_error in dangerous_urls:
            # URL ê²€ì¦ì´ êµ¬í˜„ë˜ì–´ ë‚´ë¶€ IP ì ‘ê·¼ì´ ì°¨ë‹¨ë¨
            with pytest.raises(ValueError, match=expected_error):
                scraper.fetch_page(url)

    def test_reject_file_protocol(self):
        """
        í…ŒìŠ¤íŠ¸: file:// í”„ë¡œí† ì½œ ê±°ë¶€

        ì‹œë‚˜ë¦¬ì˜¤: file:// í”„ë¡œí† ì½œì„ í†µí•œ ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ì‹œë„
        ì˜ˆìƒ ê²°ê³¼: file:// í”„ë¡œí† ì½œ ê±°ë¶€ (ValueError ë°œìƒ)
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        # URL ê²€ì¦ì´ êµ¬í˜„ë˜ì–´ file:// í”„ë¡œí† ì½œì´ ì°¨ë‹¨ë¨
        with pytest.raises(ValueError, match="í—ˆìš©ë˜ì§€ ì•ŠëŠ” í”„ë¡œí† ì½œ"):
            scraper.fetch_page("file:///etc/passwd")

    def test_reject_redirect_to_internal_network(self):
        """
        í…ŒìŠ¤íŠ¸: ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ë¡œì˜ ë¦¬ë‹¤ì´ë ‰íŠ¸ ê±°ë¶€

        ì‹œë‚˜ë¦¬ì˜¤: ì™¸ë¶€ URLì´ ë‚´ë¶€ IPë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ëŠ” ê²½ìš°
        ì˜ˆìƒ ê²°ê³¼: ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„ ìµœì¢… URLë„ ê²€ì¦ë˜ì–´ì•¼ í•˜ë©° ValueError ë°œìƒ
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œë®¬ë ˆì´ì…˜ - response.urlì„ ë¬¸ìì—´ë¡œ ì„¤ì •
            final_response = Mock()
            final_response.status_code = 200
            final_response.text = "<html>Internal</html>"
            final_response.url = "http://192.168.1.1/internal"  # ë¬¸ìì—´ë¡œ ì„¤ì •
            final_response.raise_for_status = Mock()

            mock_get.return_value = final_response

            # ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„ URL ê²€ì¦ì´ êµ¬í˜„ë˜ì–´ ë‚´ë¶€ IPë¡œì˜ ë¦¬ë‹¤ì´ë ‰íŠ¸ê°€ ì°¨ë‹¨ë¨
            with pytest.raises(ValueError, match="ë‚´ë¶€ IP ì ‘ê·¼ ê¸ˆì§€"):
                scraper.fetch_page("https://example.com/redirect")

    def test_allow_valid_external_urls(self):
        """
        í…ŒìŠ¤íŠ¸: ìœ íš¨í•œ ì™¸ë¶€ URL í—ˆìš©

        ì‹œë‚˜ë¦¬ì˜¤: ì •ìƒì ì¸ ì™¸ë¶€ ì›¹ì‚¬ì´íŠ¸ URL
        ì˜ˆìƒ ê²°ê³¼: https://example.com ë“± ì •ìƒ URLì€ í—ˆìš©
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        valid_urls = [
            "https://example.com/page",
            "https://www.google.com",
            "https://github.com/user/repo",
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        for url in valid_urls:
            with patch.object(scraper.session, 'get') as mock_get:
                mock_response = Mock()
                mock_response.status_code = 200
                mock_response.text = "<html>Valid Content</html>"
                mock_response.url = url  # response.urlì„ ë¬¸ìì—´ë¡œ ì„¤ì •
                mock_response.raise_for_status = Mock()
                mock_get.return_value = mock_response

                result = scraper.fetch_page(url)
                assert result is not None, f"{url}ì€ í—ˆìš©ë˜ì–´ì•¼ í•¨"

    def test_allow_redirect_to_valid_external_url(self):
        """
        í…ŒìŠ¤íŠ¸: ìœ íš¨í•œ ì™¸ë¶€ URLë¡œì˜ ë¦¬ë‹¤ì´ë ‰íŠ¸ í—ˆìš©

        ì‹œë‚˜ë¦¬ì˜¤: ì •ìƒì ì¸ ì™¸ë¶€ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        ì˜ˆìƒ ê²°ê³¼: ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ë˜ê³  ìµœì¢… URLë„ ê²€ì¦ë¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œë®¬ë ˆì´ì…˜
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.text = "<html>Redirected Content</html>"
            mock_response.url = "https://www.example.org/newpage"  # ë‹¤ë¥¸ ìœ íš¨í•œ ì™¸ë¶€ URL
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response

            result = scraper.fetch_page("https://example.com/redirect")
            assert result is not None, "ìœ íš¨í•œ ì™¸ë¶€ URLë¡œì˜ ë¦¬ë‹¤ì´ë ‰íŠ¸ëŠ” í—ˆìš©ë˜ì–´ì•¼ í•¨"


class TestSSLVerification:
    """SSL ê²€ì¦ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_ssl_verification_enabled_by_default(self):
        """
        í…ŒìŠ¤íŠ¸: SSL ê²€ì¦ì´ ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: HTTPS URL ìš”ì²­ ì‹œ SSL ì¸ì¦ì„œ ê²€ì¦
        ì˜ˆìƒ ê²°ê³¼: verify=Trueë¡œ ìš”ì²­ì´ ì „ì†¡ë¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.text = "<html></html>"
            mock_response.url = "https://secure-site.com"
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response

            scraper.fetch_page("https://secure-site.com")

            # requests.get í˜¸ì¶œ ì‹œ verify íŒŒë¼ë¯¸í„° í™•ì¸
            call_args = mock_get.call_args

            # verify=Trueê°€ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
            assert call_args.kwargs.get('verify') is True, "SSL ê²€ì¦ì´ ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”ë˜ì–´ì•¼ í•¨"

    def test_reject_invalid_ssl_certificate(self):
        """
        í…ŒìŠ¤íŠ¸: ì˜ëª»ëœ SSL ì¸ì¦ì„œ ê±°ë¶€

        ì‹œë‚˜ë¦¬ì˜¤: SSL ì¸ì¦ì„œê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ì‚¬ì´íŠ¸ ì ‘ê·¼
        ì˜ˆìƒ ê²°ê³¼: SSLError ë°œìƒ
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://expired.badssl.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_get.side_effect = requests.exceptions.SSLError(
                "SSL certificate verify failed"
            )

            with pytest.raises(requests.exceptions.SSLError):
                scraper.fetch_page("https://expired.badssl.com")

    def test_ssl_verification_not_disabled(self):
        """
        í…ŒìŠ¤íŠ¸: SSL ê²€ì¦ì´ ì˜ë„ì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: ì½”ë“œì— verify=Falseê°€ ì—†ëŠ”ì§€ í™•ì¸
        ì˜ˆìƒ ê²°ê³¼: verify=Trueë¡œ ëª…ì‹œì  ì„¤ì •
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.text = "<html></html>"
            mock_response.url = "https://example.com"
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response

            scraper.fetch_page("https://example.com")

            # verify=Trueê°€ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
            call_kwargs = mock_get.call_args.kwargs if mock_get.call_args.kwargs else {}
            assert call_kwargs.get('verify') is True, \
                "SSL ê²€ì¦ì´ ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”ë˜ì–´ì•¼ í•¨"

    def test_ssl_with_custom_ca_bundle(self):
        """
        í…ŒìŠ¤íŠ¸: ì»¤ìŠ¤í…€ CA ë²ˆë“¤ ì‚¬ìš© ì§€ì›

        ì‹œë‚˜ë¦¬ì˜¤: ìì²´ ì„œëª…ëœ ì¸ì¦ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        ì˜ˆìƒ ê²°ê³¼: verify íŒŒë¼ë¯¸í„°ì— CA ë²ˆë“¤ ê²½ë¡œ ì „ë‹¬ ê°€ëŠ¥
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        ca_bundle_path = "/path/to/ca-bundle.crt"

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_get.return_value = Mock(status_code=200, text="<html></html>")

            # í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•Šì§€ë§Œ, í–¥í›„ ê°œì„  ê°€ëŠ¥
            # scraper.fetch_page("https://example.com", verify=ca_bundle_path)

            # TODO: verify íŒŒë¼ë¯¸í„° ì˜µì…˜ ì¶”ê°€ ê³ ë ¤


class TestWebScraperInitialization:
    """WebScraper ì´ˆê¸°í™” ì „ìš© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_initialization_succeeds_without_errors(self):
        """
        í…ŒìŠ¤íŠ¸: ì´ˆê¸°í™”ê°€ ì—ëŸ¬ ì—†ì´ ì„±ê³µí•˜ëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: WebScraper ê°ì²´ ìƒì„± ì‹œ ZeroDivisionErrorë‚˜ ë‹¤ë¥¸ ì˜ˆì™¸ê°€ ë°œìƒí•˜ì§€ ì•ŠìŒ
        ì˜ˆìƒ ê²°ê³¼: ê°ì²´ê°€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë¨ (ì´ì „ ë²„ê·¸: line 51ì˜ 1/0)
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        base_url = "https://example.com"
        timeout = 10

        # ì´ˆê¸°í™” ì¤‘ ì˜ˆì™¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨
        scraper = WebScraper(base_url, timeout)

        # Assert (ê²€ì¦)
        assert scraper is not None

    def test_initialization_with_custom_timeout(self):
        """
        í…ŒìŠ¤íŠ¸: ì»¤ìŠ¤í…€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì´ˆê¸°í™”

        ì‹œë‚˜ë¦¬ì˜¤: ì‚¬ìš©ì ì •ì˜ íƒ€ì„ì•„ì›ƒ ê°’ìœ¼ë¡œ WebScraper ìƒì„±
        ì˜ˆìƒ ê²°ê³¼: timeoutì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë¨
        """
        # Arrange (ì¤€ë¹„)
        base_url = "https://example.com"
        custom_timeout = 15

        # Act (ì‹¤í–‰)
        scraper = WebScraper(base_url, custom_timeout)

        # Assert (ê²€ì¦)
        assert scraper.timeout == custom_timeout

    def test_initialization_with_default_timeout(self):
        """
        í…ŒìŠ¤íŠ¸: ê¸°ë³¸ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì´ˆê¸°í™”

        ì‹œë‚˜ë¦¬ì˜¤: timeout íŒŒë¼ë¯¸í„° ì—†ì´ WebScraper ìƒì„±
        ì˜ˆìƒ ê²°ê³¼: ê¸°ë³¸ê°’ 10ì´ˆë¡œ ì„¤ì •ë¨
        """
        # Arrange (ì¤€ë¹„)
        base_url = "https://example.com"

        # Act (ì‹¤í–‰)
        scraper = WebScraper(base_url)

        # Assert (ê²€ì¦)
        assert scraper.timeout == 10

    def test_all_instance_variables_properly_set(self):
        """
        í…ŒìŠ¤íŠ¸: ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: ì´ˆê¸°í™” í›„ base_url, timeout, session ëª¨ë‘ í™•ì¸
        ì˜ˆìƒ ê²°ê³¼: ëª¨ë“  ì†ì„±ì´ ì˜¬ë°”ë¥¸ ê°’ê³¼ íƒ€ì…ìœ¼ë¡œ ì„¤ì •ë¨
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        base_url = "https://example.com"
        timeout = 15
        scraper = WebScraper(base_url, timeout)

        # Assert (ê²€ì¦)
        assert scraper.base_url == base_url
        assert scraper.timeout == timeout
        assert scraper.session is not None
        assert isinstance(scraper.session, requests.Session)

    def test_session_has_user_agent_header(self):
        """
        í…ŒìŠ¤íŠ¸: ì„¸ì…˜ì— User-Agent í—¤ë”ê°€ ì„¤ì •ë˜ëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: ì´ˆê¸°í™” í›„ session.headersì— User-Agent í¬í•¨
        ì˜ˆìƒ ê²°ê³¼: User-Agent í—¤ë”ê°€ ì¡´ì¬í•˜ê³  ì ì ˆí•œ ê°’ì„ ê°€ì§
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        scraper = WebScraper("https://example.com")

        # Assert (ê²€ì¦)
        assert 'User-Agent' in scraper.session.headers
        assert 'Mozilla' in scraper.session.headers['User-Agent']

    def test_session_max_redirects_is_set(self):
        """
        í…ŒìŠ¤íŠ¸: ì„¸ì…˜ì˜ max_redirectsê°€ ì„¤ì •ë˜ëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: ì´ˆê¸°í™” í›„ session.max_redirects ê°’ í™•ì¸
        ì˜ˆìƒ ê²°ê³¼: MAX_REDIRECTS ìƒìˆ˜ ê°’(5)ìœ¼ë¡œ ì„¤ì •ë¨
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        scraper = WebScraper("https://example.com")

        # Assert (ê²€ì¦)
        assert scraper.session.max_redirects == WebScraper.MAX_REDIRECTS
        assert scraper.session.max_redirects == 5

    def test_context_manager_enter(self):
        """
        í…ŒìŠ¤íŠ¸: Context managerì˜ __enter__ ë©”ì„œë“œ

        ì‹œë‚˜ë¦¬ì˜¤: with ë¬¸ìœ¼ë¡œ WebScraper ì‚¬ìš© ì‹œì‘
        ì˜ˆìƒ ê²°ê³¼: __enter__ê°€ selfë¥¼ ë°˜í™˜í•˜ê³  ì •ìƒ ì‘ë™
        """
        # Arrange (ì¤€ë¹„)
        base_url = "https://example.com"

        # Act (ì‹¤í–‰)
        with WebScraper(base_url) as scraper:
            # Assert (ê²€ì¦)
            assert scraper is not None
            assert scraper.base_url == base_url
            assert scraper.session is not None

    def test_context_manager_exit_closes_session(self):
        """
        í…ŒìŠ¤íŠ¸: Context managerì˜ __exit__ ë©”ì„œë“œê°€ ì„¸ì…˜ì„ ë‹«ëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: with ë¸”ë¡ ì¢…ë£Œ ì‹œ session.close() í˜¸ì¶œ
        ì˜ˆìƒ ê²°ê³¼: ì„¸ì…˜ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act (ì‹¤í–‰)
        with patch.object(scraper.session, 'close') as mock_close:
            scraper.__exit__(None, None, None)

            # Assert (ê²€ì¦)
            mock_close.assert_called_once()

    def test_context_manager_full_lifecycle(self):
        """
        í…ŒìŠ¤íŠ¸: Context managerì˜ ì „ì²´ ìƒëª…ì£¼ê¸°

        ì‹œë‚˜ë¦¬ì˜¤: with ë¬¸ìœ¼ë¡œ ì „ì²´ ì‚¬ìš© ê³¼ì • í…ŒìŠ¤íŠ¸
        ì˜ˆìƒ ê²°ê³¼: ì§„ì…, ì‚¬ìš©, ì¢…ë£Œê°€ ëª¨ë‘ ì •ìƒ ì‘ë™
        """
        # Arrange (ì¤€ë¹„)
        base_url = "https://example.com"

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with WebScraper(base_url) as scraper:
            # ì§„ì… í›„: ì •ìƒì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
            assert scraper.base_url == base_url
            assert scraper.session is not None

            # ì„¸ì…˜ close ê°ì‹œ
            with patch.object(scraper.session, 'close') as mock_close:
                pass
        # with ë¸”ë¡ ì¢…ë£Œ í›„: closeê°€ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ëŠ” ë¸”ë¡ ë‚´ë¶€ì—ì„œ í™•ì¸ ë¶ˆê°€
        # í•˜ì§€ë§Œ ì˜ˆì™¸ ì—†ì´ ì •ìƒ ì¢…ë£Œë˜ì–´ì•¼ í•¨

    def test_multiple_instances_independent(self):
        """
        í…ŒìŠ¤íŠ¸: ì—¬ëŸ¬ WebScraper ì¸ìŠ¤í„´ìŠ¤ê°€ ë…ë¦½ì ì¸ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: ë‘ ê°œì˜ WebScraper ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
        ì˜ˆìƒ ê²°ê³¼: ê° ì¸ìŠ¤í„´ìŠ¤ê°€ ë…ë¦½ì ì¸ sessionê³¼ ì†ì„±ì„ ê°€ì§
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        scraper1 = WebScraper("https://example1.com", timeout=10)
        scraper2 = WebScraper("https://example2.com", timeout=20)

        # Assert (ê²€ì¦)
        assert scraper1.base_url != scraper2.base_url
        assert scraper1.timeout != scraper2.timeout
        assert scraper1.session is not scraper2.session

    def test_initialization_does_not_make_network_requests(self):
        """
        í…ŒìŠ¤íŠ¸: ì´ˆê¸°í™” ì‹œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì´ ë°œìƒí•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸

        ì‹œë‚˜ë¦¬ì˜¤: WebScraper ìƒì„±ë§Œìœ¼ë¡œëŠ” HTTP ìš”ì²­ì´ ë°œìƒí•˜ì§€ ì•ŠìŒ
        ì˜ˆìƒ ê²°ê³¼: ì´ˆê¸°í™” ì¤‘ requests.get()ì´ í˜¸ì¶œë˜ì§€ ì•ŠìŒ
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        with patch('requests.Session.get') as mock_get:
            scraper = WebScraper("https://example.com")

            # Assert (ê²€ì¦)
            mock_get.assert_not_called()
            assert scraper is not None


class TestNormalFunctionality:
    """ì •ìƒ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_webscraper_initialization(self):
        """
        í…ŒìŠ¤íŠ¸: WebScraper ì´ˆê¸°í™”

        ì‹œë‚˜ë¦¬ì˜¤: ì •ìƒì ì¸ WebScraper ê°ì²´ ìƒì„±
        ì˜ˆìƒ ê²°ê³¼: base_url, timeout, sessionì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë¨
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        base_url = "https://example.com"
        timeout = 15
        scraper = WebScraper(base_url, timeout)

        # Assert (ê²€ì¦)
        assert scraper.base_url == base_url
        assert scraper.timeout == timeout
        assert scraper.session is not None
        assert 'User-Agent' in scraper.session.headers

    def test_fetch_page_success(self):
        """
        í…ŒìŠ¤íŠ¸: í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ

        ì‹œë‚˜ë¦¬ì˜¤: ì •ìƒì ì¸ HTTP 200 ì‘ë‹µ
        ì˜ˆìƒ ê²°ê³¼: HTML í…ìŠ¤íŠ¸ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        expected_html = "<html><body>Test</body></html>"

        # Act (ì‹¤í–‰)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.text = expected_html
            mock_response.url = "https://example.com/test"  # response.urlì„ ë¬¸ìì—´ë¡œ ì„¤ì •
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response

            result = scraper.fetch_page("https://example.com/test")

        # Assert (ê²€ì¦)
        assert result == expected_html
        mock_get.assert_called_once()

    def test_fetch_page_timeout(self):
        """
        í…ŒìŠ¤íŠ¸: í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° íƒ€ì„ì•„ì›ƒ

        ì‹œë‚˜ë¦¬ì˜¤: ìš”ì²­ì´ timeout ì‹œê°„ì„ ì´ˆê³¼
        ì˜ˆìƒ ê²°ê³¼: Timeout ì˜ˆì™¸ ë°œìƒ
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com", timeout=1)

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_get.side_effect = requests.exceptions.Timeout()

            with pytest.raises(requests.exceptions.Timeout):
                scraper.fetch_page("https://slow-site.com")

    def test_fetch_page_404_error(self):
        """
        í…ŒìŠ¤íŠ¸: 404 ì—ëŸ¬ ì²˜ë¦¬

        ì‹œë‚˜ë¦¬ì˜¤: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í˜ì´ì§€ ìš”ì²­
        ì˜ˆìƒ ê²°ê³¼: HTTPError ì˜ˆì™¸ ë°œìƒ
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 404
            mock_response.url = "https://example.com/notfound"  # response.urlì„ ë¬¸ìì—´ë¡œ ì„¤ì •
            mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError()
            mock_get.return_value = mock_response

            with pytest.raises(requests.exceptions.HTTPError):
                scraper.fetch_page("https://example.com/notfound")

    def test_parse_articles_success(self):
        """
        í…ŒìŠ¤íŠ¸: ê¸°ì‚¬ íŒŒì‹± ì„±ê³µ

        ì‹œë‚˜ë¦¬ì˜¤: ìœ íš¨í•œ HTMLì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
        ì˜ˆìƒ ê²°ê³¼: Article ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        html = """
        <html>
            <body>
                <article class="post">
                    <h2 class="title">Test Article</h2>
                    <a href="/article/1">Read More</a>
                    <p class="summary">This is a test summary.</p>
                    <time datetime="2024-01-01">January 1, 2024</time>
                </article>
            </body>
        </html>
        """

        # Act (ì‹¤í–‰)
        articles = scraper.parse_articles(html)

        # Assert (ê²€ì¦)
        assert len(articles) == 1
        assert articles[0].title == "Test Article"
        assert articles[0].link == "/article/1"
        assert articles[0].summary == "This is a test summary."
        assert articles[0].published_date == "2024-01-01"

    def test_scrape_multiple_pages(self):
        """
        í…ŒìŠ¤íŠ¸: ì—¬ëŸ¬ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

        ì‹œë‚˜ë¦¬ì˜¤: page_count=3ìœ¼ë¡œ ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘
        ì˜ˆìƒ ê²°ê³¼: ëª¨ë“  í˜ì´ì§€ì˜ ê¸°ì‚¬ê°€ í•©ì³ì§„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        def mock_fetch(url):
            return """
            <html>
                <body>
                    <article class="post">
                        <h2 class="title">Article</h2>
                        <a href="/test">Link</a>
                    </article>
                </body>
            </html>
            """

        # Act (ì‹¤í–‰)
        with patch.object(scraper, 'fetch_page', side_effect=mock_fetch):
            articles = scraper.scrape(page_count=3)

        # Assert (ê²€ì¦)
        assert len(articles) == 3, "3ê°œ í˜ì´ì§€ì—ì„œ ê° 1ê°œì”© ì´ 3ê°œ ê¸°ì‚¬"

    def test_extract_links_function(self):
        """
        í…ŒìŠ¤íŠ¸: ë§í¬ ì¶”ì¶œ í•¨ìˆ˜

        ì‹œë‚˜ë¦¬ì˜¤: HTMLì—ì„œ ëª¨ë“  ë§í¬ ì¶”ì¶œ
        ì˜ˆìƒ ê²°ê³¼: href ì†ì„±ì„ ê°€ì§„ ëª¨ë“  a íƒœê·¸ì˜ ë§í¬ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        html = """
        <html>
            <body>
                <a href="/relative">Relative Link</a>
                <a href="https://external.com">External Link</a>
                <a href="/another">Another Link</a>
            </body>
        </html>
        """
        base_url = "https://example.com"

        # Act (ì‹¤í–‰)
        links = extract_links(html, base_url)

        # Assert (ê²€ì¦)
        assert len(links) == 3
        assert "https://example.com/relative" in links
        assert "https://external.com" in links
        assert "https://example.com/another" in links

    def test_extract_images_function(self):
        """
        í…ŒìŠ¤íŠ¸: ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜

        ì‹œë‚˜ë¦¬ì˜¤: HTMLì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
        ì˜ˆìƒ ê²°ê³¼: srcì™€ alt ì†ì„±ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        html = """
        <html>
            <body>
                <img src="/image1.jpg" alt="Image 1">
                <img src="https://cdn.com/image2.png" alt="Image 2">
                <img src="/image3.gif">
            </body>
        </html>
        """
        base_url = "https://example.com"

        # Act (ì‹¤í–‰)
        images = extract_images(html, base_url)

        # Assert (ê²€ì¦)
        assert len(images) == 3
        assert images[0]['src'] == "https://example.com/image1.jpg"
        assert images[0]['alt'] == "Image 1"
        assert images[1]['src'] == "https://cdn.com/image2.png"
        assert images[2]['alt'] == ""

    def test_session_close(self):
        """
        í…ŒìŠ¤íŠ¸: ì„¸ì…˜ ì¢…ë£Œ

        ì‹œë‚˜ë¦¬ì˜¤: scraper.close() í˜¸ì¶œ
        ì˜ˆìƒ ê²°ê³¼: session.close()ê°€ í˜¸ì¶œë¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Act (ì‹¤í–‰)
        with patch.object(scraper.session, 'close') as mock_close:
            scraper.close()

        # Assert (ê²€ì¦)
        mock_close.assert_called_once()

    def test_article_dataclass(self):
        """
        í…ŒìŠ¤íŠ¸: Article ë°ì´í„°í´ë˜ìŠ¤

        ì‹œë‚˜ë¦¬ì˜¤: Article ê°ì²´ ìƒì„± ë° ì†ì„± í™•ì¸
        ì˜ˆìƒ ê²°ê³¼: ëª¨ë“  í•„ë“œê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë¨
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        article = Article(
            title="Test Title",
            link="https://example.com/article",
            summary="Test Summary",
            published_date="2024-01-01"
        )

        # Assert (ê²€ì¦)
        assert article.title == "Test Title"
        assert article.link == "https://example.com/article"
        assert article.summary == "Test Summary"
        assert article.published_date == "2024-01-01"

    def test_article_optional_fields(self):
        """
        í…ŒìŠ¤íŠ¸: Articleì˜ ì„ íƒì  í•„ë“œ

        ì‹œë‚˜ë¦¬ì˜¤: summaryì™€ published_date ì—†ì´ Article ìƒì„±
        ì˜ˆìƒ ê²°ê³¼: ì„ íƒì  í•„ë“œëŠ” Noneìœ¼ë¡œ ì„¤ì •ë¨
        """
        # Arrange & Act (ì¤€ë¹„ ë° ì‹¤í–‰)
        article = Article(
            title="Minimal Article",
            link="/article"
        )

        # Assert (ê²€ì¦)
        assert article.title == "Minimal Article"
        assert article.link == "/article"
        assert article.summary is None
        assert article.published_date is None


class TestEdgeCases:
    """ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_scrape_with_partial_page_failure(self):
        """
        í…ŒìŠ¤íŠ¸: ì¼ë¶€ í˜ì´ì§€ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬

        ì‹œë‚˜ë¦¬ì˜¤: ì—¬ëŸ¬ í˜ì´ì§€ ì¤‘ ì¼ë¶€ë§Œ ì„±ê³µ
        ì˜ˆìƒ ê²°ê³¼: ì„±ê³µí•œ í˜ì´ì§€ì˜ ê¸°ì‚¬ë§Œ ë°˜í™˜
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        def mock_fetch(url):
            if "page=2" in url:
                raise requests.exceptions.HTTPError("Page not found")
            return """
            <html>
                <body>
                    <article class="post">
                        <h2 class="title">Article</h2>
                        <a href="/test">Link</a>
                    </article>
                </body>
            </html>
            """

        # Act (ì‹¤í–‰)
        with patch.object(scraper, 'fetch_page', side_effect=mock_fetch):
            articles = scraper.scrape(page_count=3)

        # Assert (ê²€ì¦)
        # 3ê°œ í˜ì´ì§€ ì¤‘ 1ê°œ ì‹¤íŒ¨, 2ê°œ ì„±ê³µ
        assert len(articles) == 2

    def test_parse_articles_with_unicode_and_whitespace(self):
        """
        í…ŒìŠ¤íŠ¸: ìœ ë‹ˆì½”ë“œ ë° ê³µë°± ì²˜ë¦¬

        ì‹œë‚˜ë¦¬ì˜¤: ì œëª©ê³¼ ìš”ì•½ì— ì—¬ëŸ¬ ê³µë°±ê³¼ ìœ ë‹ˆì½”ë“œ ë¬¸ì
        ì˜ˆìƒ ê²°ê³¼: strip()ìœ¼ë¡œ ê³µë°±ì´ ì œê±°ë˜ê³  ìœ ë‹ˆì½”ë“œê°€ ë³´ì¡´ë¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")
        html = """
        <html>
            <body>
                <article class="post">
                    <h2 class="title">

                        í•œê¸€ Title   with   spaces

                    </h2>
                    <a href="/test">Link</a>
                    <p class="summary">  Summary  with  spaces  æ—¥æœ¬èª  </p>
                </article>
            </body>
        </html>
        """

        # Act (ì‹¤í–‰)
        articles = scraper.parse_articles(html)

        # Assert (ê²€ì¦)
        assert len(articles) == 1
        assert articles[0].title == "í•œê¸€ Title   with   spaces"
        assert "æ—¥æœ¬èª" in articles[0].summary
        assert not articles[0].summary.startswith(" ")
        assert not articles[0].summary.endswith(" ")

    def test_very_large_html_parsing(self):
        """
        í…ŒìŠ¤íŠ¸: ë§¤ìš° í° HTML íŒŒì‹±

        ì‹œë‚˜ë¦¬ì˜¤: ìˆ˜ë°± ê°œì˜ ê¸°ì‚¬ê°€ ìˆëŠ” í° HTML
        ì˜ˆìƒ ê²°ê³¼: ëª¨ë“  ê¸°ì‚¬ê°€ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±ë¨
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # 100ê°œì˜ ê¸°ì‚¬ ìƒì„±
        articles_html = ""
        for i in range(100):
            articles_html += f"""
                <article class="post">
                    <h2 class="title">Article {i}</h2>
                    <a href="/article/{i}">Link {i}</a>
                </article>
            """

        html = f"<html><body>{articles_html}</body></html>"

        # Act (ì‹¤í–‰)
        articles = scraper.parse_articles(html)

        # Assert (ê²€ì¦)
        assert len(articles) == 100
        assert articles[0].title == "Article 0"
        assert articles[99].title == "Article 99"

    @patch('socket.getaddrinfo')
    def test_network_connection_error(self, mock_getaddrinfo):
        """
        í…ŒìŠ¤íŠ¸: ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜

        ì‹œë‚˜ë¦¬ì˜¤: ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨
        ì˜ˆìƒ ê²°ê³¼: ConnectionError ë°œìƒ
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Mock DNS resolution to return an external IP (to bypass DNS Rebinding protection)
        mock_getaddrinfo.return_value = [
            (2, 1, 6, '', ('93.184.216.34', 0))  # example.comì˜ ì‹¤ì œ IP
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_get.side_effect = requests.exceptions.ConnectionError()

            with pytest.raises(requests.exceptions.ConnectionError):
                scraper.fetch_page("https://unreachable.com")


class TestDNSRebindingProtection:
    """DNS Rebinding ë³´í˜¸ í…ŒìŠ¤íŠ¸"""

    @patch('socket.getaddrinfo')
    def test_dns_rebinding_to_private_ip(self, mock_getaddrinfo):
        """
        í…ŒìŠ¤íŠ¸: DNS Rebinding - ë„ë©”ì¸ì´ ë‚´ë¶€ IPë¡œ í•´ì„ë˜ëŠ” ê²½ìš°

        ì‹œë‚˜ë¦¬ì˜¤: ì™¸ë¶€ ë„ë©”ì¸ ì´ë¦„ì´ DNSì—ì„œ ë‚´ë¶€ IPë¡œ í•´ì„ë¨
        ì˜ˆìƒ ê²°ê³¼: ValueError ë°œìƒ (DNS Rebinding ê°ì§€)
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Mock DNS resolution to return a private IP
        mock_getaddrinfo.return_value = [
            (2, 1, 6, '', ('192.168.1.100', 0))
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with pytest.raises(ValueError, match="DNS Rebinding ê°ì§€.*ë‚´ë¶€ IP"):
            scraper.fetch_page("https://malicious-domain.com")

    @patch('socket.getaddrinfo')
    def test_dns_rebinding_to_loopback(self, mock_getaddrinfo):
        """
        í…ŒìŠ¤íŠ¸: DNS Rebinding - ë„ë©”ì¸ì´ ë£¨í”„ë°± ì£¼ì†Œë¡œ í•´ì„ë˜ëŠ” ê²½ìš°

        ì‹œë‚˜ë¦¬ì˜¤: ì™¸ë¶€ ë„ë©”ì¸ ì´ë¦„ì´ 127.0.0.1ë¡œ í•´ì„ë¨
        ì˜ˆìƒ ê²°ê³¼: ValueError ë°œìƒ (DNS Rebinding ê°ì§€)
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Mock DNS resolution to return loopback address
        mock_getaddrinfo.return_value = [
            (2, 1, 6, '', ('127.0.0.1', 0))
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with pytest.raises(ValueError, match="DNS Rebinding ê°ì§€.*ë£¨í”„ë°±"):
            scraper.fetch_page("https://evil.example.com")

    @patch('socket.getaddrinfo')
    def test_dns_rebinding_to_link_local(self, mock_getaddrinfo):
        """
        í…ŒìŠ¤íŠ¸: DNS Rebinding - ë„ë©”ì¸ì´ ë§í¬ ë¡œì»¬ ì£¼ì†Œë¡œ í•´ì„ë˜ëŠ” ê²½ìš°

        ì‹œë‚˜ë¦¬ì˜¤: ì™¸ë¶€ ë„ë©”ì¸ ì´ë¦„ì´ 169.254.x.xë¡œ í•´ì„ë¨
        ì˜ˆìƒ ê²°ê³¼: ValueError ë°œìƒ (DNS Rebinding ê°ì§€)
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Mock DNS resolution to return link-local address
        mock_getaddrinfo.return_value = [
            (2, 1, 6, '', ('169.254.1.1', 0))
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with pytest.raises(ValueError, match="DNS Rebinding ê°ì§€.*ë§í¬ ë¡œì»¬"):
            scraper.fetch_page("https://linklocal-attack.com")

    @patch('socket.getaddrinfo')
    def test_dns_rebinding_to_aws_metadata(self, mock_getaddrinfo):
        """
        í…ŒìŠ¤íŠ¸: DNS Rebinding - ë„ë©”ì¸ì´ AWS ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸ë¡œ í•´ì„ë˜ëŠ” ê²½ìš°

        ì‹œë‚˜ë¦¬ì˜¤: ì™¸ë¶€ ë„ë©”ì¸ ì´ë¦„ì´ 169.254.169.254ë¡œ í•´ì„ë¨
        ì˜ˆìƒ ê²°ê³¼: ValueError ë°œìƒ (í´ë¼ìš°ë“œ ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸ ì ‘ê·¼ ê¸ˆì§€)
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Mock DNS resolution to return AWS metadata endpoint
        mock_getaddrinfo.return_value = [
            (2, 1, 6, '', ('169.254.169.254', 0))
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with pytest.raises(ValueError, match="DNS Rebinding ê°ì§€.*ë©”íƒ€ë°ì´í„°"):
            scraper.fetch_page("https://aws-metadata-attack.com")

    @patch('socket.getaddrinfo')
    def test_dns_rebinding_allows_valid_external_ip(self, mock_getaddrinfo):
        """
        í…ŒìŠ¤íŠ¸: DNS Rebinding ë³´í˜¸ê°€ ì •ìƒì ì¸ ì™¸ë¶€ IPëŠ” í—ˆìš©í•˜ëŠ”ì§€ ê²€ì¦

        ì‹œë‚˜ë¦¬ì˜¤: ì™¸ë¶€ ë„ë©”ì¸ ì´ë¦„ì´ ì •ìƒì ì¸ ì™¸ë¶€ IPë¡œ í•´ì„ë¨
        ì˜ˆìƒ ê²°ê³¼: ì •ìƒì ìœ¼ë¡œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì§„í–‰
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Mock DNS resolution to return a valid external IP
        mock_getaddrinfo.return_value = [
            (2, 1, 6, '', ('93.184.216.34', 0))  # example.comì˜ ì‹¤ì œ IP
        ]

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.text = "<html>Valid Content</html>"
            mock_response.url = "https://example.com"
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response

            result = scraper.fetch_page("https://example.com")
            assert result == "<html>Valid Content</html>"

    @patch('socket.getaddrinfo')
    def test_dns_resolution_failure_allows_request(self, mock_getaddrinfo):
        """
        í…ŒìŠ¤íŠ¸: DNS í•´ì„ ì‹¤íŒ¨ ì‹œì—ë„ ìš”ì²­ì´ ì§„í–‰ë˜ëŠ”ì§€ ê²€ì¦

        ì‹œë‚˜ë¦¬ì˜¤: DNS í•´ì„ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° (ì˜¤í”„ë¼ì¸ í™˜ê²½ ë“±)
        ì˜ˆìƒ ê²°ê³¼: DNS ê²€ì¦ì„ ê±´ë„ˆë›°ê³  ìš”ì²­ ì§„í–‰ (ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ë‚˜ì¤‘ì— ë°œìƒ)
        """
        # Arrange (ì¤€ë¹„)
        scraper = WebScraper("https://example.com")

        # Mock DNS resolution to fail
        mock_getaddrinfo.side_effect = socket.gaierror("Name or service not known")

        # Act & Assert (ì‹¤í–‰ ë° ê²€ì¦)
        with patch.object(scraper.session, 'get') as mock_get:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.text = "<html>Content</html>"
            mock_response.url = "https://example.com"
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response

            # DNS ì‹¤íŒ¨ëŠ” ë¬´ì‹œë˜ê³  ìš”ì²­ì´ ì§„í–‰ë¨
            result = scraper.fetch_page("https://example.com")
            assert result == "<html>Content</html>"
